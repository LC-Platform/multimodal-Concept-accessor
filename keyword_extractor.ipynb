{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPr2uTtznISMxwdnhcMLYqO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC-Platform/multimodal-Concept-accessor/blob/Domain-keyword-identifier/keyword_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DOMAIN KEYWORD EXTRACTION USING DIFFERENT TECHNOQUES**"
      ],
      "metadata": {
        "id": "J2opxSuINUYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF WITH THRESHOLD"
      ],
      "metadata": {
        "id": "hn6UrhdCnqB-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmNQMAERjDJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF scikit-learn nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHTJk4qezpep",
        "outputId": "f67e123a-d48d-45c0-9262-7affc7c6c332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_qItWka0hmLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63aa3ff9-8e30-49d6-8679-09c50623a4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLANT KINGDOM\n",
            "29\n",
            "In the previous chapter, we looked at the broad classification of living\n",
            "organisms under the system proposed by Whittaker (1969) wherein he\n",
            "suggested the Five Kingdom classification viz. Monera, Protista, Fungi,\n",
            "Animalia and Plantae.  In this chapter, we will deal in detail with further\n",
            "classification within Kingdom Plantae popularly known as the ‘plant\n",
            "kingdom’.\n",
            "We must stress here that our understanding of the plant kingdom\n",
            "has changed over time. Fungi, and members of the Mone\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "pdf_path = \"/content/Class-XI-Biology.pdf\"  # replace with your PDF file path\n",
        "raw_text = extract_text_from_pdf(pdf_path)\n",
        "print(raw_text[:500])  # print first 500 characters to verify\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PHASE 1:** TF-IDF with removal of stopwords in the preprocesing step with an add on of threshold"
      ],
      "metadata": {
        "id": "B-A9nKiwHsod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text2(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # remove punctuation and non-words\n",
        "    tokens = text.split()\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "cleaned_text = preprocess_text2(raw_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S5w7gY7Hjoo",
        "outputId": "d792514d-0304-4cb5-f108-66d618d14c2b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf2(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    return dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "tfidf_dict = calculate_tfidf2(cleaned_text)\n"
      ],
      "metadata": {
        "id": "vdrzXy6XH07x"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords2(tfidf_dict, threshold=0.08):\n",
        "    keywords = {word: score for word, score in tfidf_dict.items() if score >= threshold}\n",
        "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "filtered_keywords = filter_keywords2(tfidf_dict, threshold=0.08)\n"
      ],
      "metadata": {
        "id": "Rt3GPqCrz7ID"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `filtered_keywords` is a list of (word, score) tuples\n",
        "output_path = \"tfidf_keywords_phase2.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(\"domain keywords based on TF-IDF:\\n\")\n",
        "    for word, score in filtered_keywords:\n",
        "        file.write(f\"{word}: {score:.4f}\\n\")\n",
        "\n",
        "print(f\"TF-IDF keywords saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsWQIvmAGUtE",
        "outputId": "cd5b6853-dd5f-45f9-e8c3-8dcb964c0989"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF keywords saved to tfidf_keywords_phase2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **PHASE 2:**TF-IDF with threshold set to 0.1 removed the stopwords with POS in the data preprocessing step"
      ],
      "metadata": {
        "id": "c9yOnPjjHm8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_with_pos(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Keep only nouns and proper nouns, and remove stopwords\n",
        "    filtered_tokens = [\n",
        "        word for word, pos in tagged_tokens\n",
        "        if word not in stop_words and pos in ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    ]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "cleaned_text = preprocess_text_with_pos(raw_text)"
      ],
      "metadata": {
        "id": "ZccH3WE3obrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3dd6c86-347d-49c5-9794-ee9006e645a9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    return dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "tfidf_dict = calculate_tfidf(cleaned_text)\n"
      ],
      "metadata": {
        "id": "i0IiBRdUocel"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords(tfidf_dict, threshold=0.1):\n",
        "    keywords = {word: score for word, score in tfidf_dict.items() if score >= threshold}\n",
        "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "filtered_keywords = filter_keywords(tfidf_dict, threshold=0.1)\n"
      ],
      "metadata": {
        "id": "sPghfouuofko"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `filtered_keywords` is a list of (word, score) tuples\n",
        "output_path = \"tfidf_keywords.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(\"Top domain keywords based on TF-IDF:\\n\")\n",
        "    for word, score in filtered_keywords:\n",
        "        file.write(f\"{word}: {score:.4f}\\n\")\n",
        "\n",
        "print(f\"TF-IDF keywords saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "7zP4yIiSopn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae59c215-c9ed-4557-d853-ac9afe8962bf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF keywords saved to tfidf_keywords.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PHASE 3:**TF-IDF with threshold set to 0.08 removed the stopwords with POS in the data preprocessing step"
      ],
      "metadata": {
        "id": "GxumKi5SIQCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_with_pos1(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Keep only nouns and proper nouns, and remove stopwords\n",
        "    filtered_tokens = [\n",
        "        word for word, pos in tagged_tokens\n",
        "        if word not in stop_words and pos in ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    ]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "cleaned_text = preprocess_text_with_pos1(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBILYzfGiSe",
        "outputId": "f822c318-b75b-48c5-c196-798e67c390a4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf3(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    return dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "tfidf_dict = calculate_tfidf3(cleaned_text)\n"
      ],
      "metadata": {
        "id": "jvgPQrRPIZf7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords3(tfidf_dict, threshold=0.08):\n",
        "    keywords = {word: score for word, score in tfidf_dict.items() if score >= threshold}\n",
        "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "filtered_keywords = filter_keywords3(tfidf_dict, threshold=0.08)\n"
      ],
      "metadata": {
        "id": "CH-Nuz2nIfkW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `filtered_keywords` is a list of (word, score) tuples\n",
        "output_path = \"tfidf_keywords_3.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(\"domain keywords based on TF-IDF:\\n\")\n",
        "    for word, score in filtered_keywords:\n",
        "        file.write(f\"{word}: {score:.4f}\\n\")\n",
        "\n",
        "print(f\"TF-IDF keywords saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYAj3BMdImFx",
        "outputId": "b43204c1-28a0-4308-b6c8-e314b1981ae4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF keywords saved to tfidf_keywords_3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **PHASE 4:**TF-IDF with threshold set to 0.08 removed the stopwords with POS in the data preprocessing step and inserted the lemmatization to avoid the plurals as different word"
      ],
      "metadata": {
        "id": "vaRpGix2JQnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_with_pos_and_lemmatization(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatization: Keep only nouns and proper nouns and remove stopwords\n",
        "    filtered_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word, pos in tagged_tokens\n",
        "        if word not in stop_words and pos in ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    ]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "cleaned_text4 = preprocess_text_with_pos_and_lemmatization(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFtN_MZJIu6K",
        "outputId": "783aad1d-f47e-45b8-a63a-6f1d06680d7b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf4(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    return dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "tfidf_dict = calculate_tfidf4(cleaned_text4)\n"
      ],
      "metadata": {
        "id": "klq4joAMJWHc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords4(tfidf_dict, threshold=0.08):\n",
        "    keywords = {word: score for word, score in tfidf_dict.items() if score >= threshold}\n",
        "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "filtered_keywords = filter_keywords4(tfidf_dict, threshold=0.08)\n"
      ],
      "metadata": {
        "id": "ctsCRgpdJmyU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `filtered_keywords` is a list of (word, score) tuples\n",
        "output_path = \"tfidf_keywords_4.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(\"domain keywords based on TF-IDF:\\n\")\n",
        "    for word, score in filtered_keywords:\n",
        "        file.write(f\"{word}: {score:.4f}\\n\")\n",
        "\n",
        "print(f\"TF-IDF keywords saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcuRrwQoJrYx",
        "outputId": "01b8423e-0c62-47d6-b294-cd0d6b78cc5c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF keywords saved to tfidf_keywords_4.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PMI WITH TF-IDF"
      ],
      "metadata": {
        "id": "NWU5wgGONw7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text_with_pos_and_lemmatization2(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatization: Keep only nouns and proper nouns and remove stopwords\n",
        "    filtered_tokens = [\n",
        "        lemmatizer.lemmatize(word) for word, pos in tagged_tokens\n",
        "        if word not in stop_words and pos in ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    ]\n",
        "\n",
        "    return filtered_tokens\n",
        "cleaned_tokens = preprocess_text_with_pos_and_lemmatization2(raw_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei_x5vz5N9EK",
        "outputId": "53e4df20-e053-47d1-8f5b-a36103b7a869"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate PMI for word pairs\n",
        "def calculate_pmi(tokens, window_size=5):\n",
        "    word_counts = Counter(tokens)\n",
        "    total_words = len(tokens)\n",
        "    cooccurrence = Counter()\n",
        "\n",
        "    # Sliding window to calculate co-occurrence of word pairs\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(i + 1, min(i + window_size, len(tokens))):\n",
        "            if tokens[i] != tokens[j]:\n",
        "                cooccurrence[(tokens[i], tokens[j])] += 1\n",
        "\n",
        "    pmi_dict = {}\n",
        "    for (word1, word2), count in cooccurrence.items():\n",
        "        p_x_y = count / total_words\n",
        "        p_x = word_counts[word1] / total_words\n",
        "        p_y = word_counts[word2] / total_words\n",
        "        if p_x * p_y > 0:\n",
        "            pmi_dict[(word1, word2)] = math.log(p_x_y / (p_x * p_y))\n",
        "    return pmi_dict\n",
        "pmi_dict = calculate_pmi(cleaned_tokens, window_size=5)"
      ],
      "metadata": {
        "id": "DOvG8g7xO_iU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Calculate TF-IDF\n",
        "def calculate_tfidf5(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    return dict(zip(feature_names, tfidf_scores))\n",
        "\n",
        "tfidf_dict = calculate_tfidf4(' '.join(cleaned_tokens))\n"
      ],
      "metadata": {
        "id": "kkwaD83_Pksi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords4(tfidf_dict, threshold=0.08):\n",
        "    keywords = {word: score for word, score in tfidf_dict.items() if score >= threshold}\n",
        "    return sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "filtered_keywords = filter_keywords4(tfidf_dict, threshold=0.08)"
      ],
      "metadata": {
        "id": "C10xFQRePSS3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"tfidf_keywords_with_pmi.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(\"Domain keywords based on TF-IDF and PMI:\\n\")\n",
        "\n",
        "    # Write TF-IDF results\n",
        "    file.write(\"\\nTF-IDF Keywords:\\n\")\n",
        "    for word, score in filtered_keywords:\n",
        "        file.write(f\"{word}: {score:.4f}\\n\")\n",
        "\n",
        "    # Write PMI results\n",
        "    file.write(\"\\nPMI Results:\\n\")\n",
        "    for (word1, word2), pmi in pmi_dict.items():\n",
        "        file.write(f\"PMI({word1}, {word2}): {pmi:.4f}\\n\")\n",
        "\n",
        "print(f\"TF-IDF keywords with PMI saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCHyMGGzQUGr",
        "outputId": "750dcd46-4693-4f60-97b6-779c1a16040f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF keywords with PMI saved to tfidf_keywords_with_pmi.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPACY**"
      ],
      "metadata": {
        "id": "iJk28PwHxaR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install necessary packages\n",
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
        "!pip install PyMuPDF  # for PDF reading\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KyX0uF9dQYOH",
        "outputId": "93a8bc9c-d644-42a3-edf3-056904435b04"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\n",
            "Collecting spacy<3.8.0,>=3.7.0 (from scispacy)\n",
            "  Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.15.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (2.13.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.0->scispacy)\n",
            "  Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.13.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.1.8)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
            "Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-sci-scibert 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.7.5 which is incompatible.\n",
            "spacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.51.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.7.5 thinc-8.2.5\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz (417.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_core_sci_scibert==0.5.1)\n",
            "  Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (from en_core_sci_scibert==0.5.1) (1.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1)\n",
            "  Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.5.0)\n",
            "Collecting transformers<4.27.0,>=3.4.0 (from spacy-transformers->en_core_sci_scibert==0.5.1)\n",
            "  Using cached transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (0.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.31.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2024.11.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (8.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.2.1)\n",
            "Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers, thinc, spacy\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.4.4 thinc-8.1.12 tokenizers-0.13.3 transformers-4.26.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "89fe14a4bbec41dfb5139790ada87172"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5 --no-cache-dir\n",
        "!pip install -U spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tle7PESsmhN7",
        "outputId": "537fc27a-8a25-4467-a9c4-b673328e681a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyAvIWkFoQgn",
        "outputId": "530b7675-1110-4268-9aef-e36b4dba34ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nJ-SNZpobNI",
        "outputId": "c4ffd983-e84d-42bd-ec5b-4adcd9678a12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.11/dist-packages (0.5.5)\n",
            "Collecting spacy<3.8.0,>=3.7.0 (from scispacy)\n",
            "  Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.15.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (2.13.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.0->scispacy)\n",
            "  Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.13.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.1.8)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
            "Using cached spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-sci-scibert 0.5.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.7.5 thinc-8.2.5\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz (417.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_core_sci_scibert==0.5.1)\n",
            "  Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.11/dist-packages (from en_core_sci_scibert==0.5.1) (1.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1)\n",
            "  Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.10.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.5.0)\n",
            "Requirement already satisfied: transformers<4.27.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (4.26.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from spacy-transformers->en_core_sci_scibert==0.5.1) (0.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.11/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.31.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (0.13.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (8.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers<4.27.0,>=3.4.0->spacy-transformers->en_core_sci_scibert==0.5.1) (1.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_scibert==0.5.1) (1.2.1)\n",
            "Using cached spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Using cached thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.4.4 thinc-8.1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import libraries\n",
        "import spacy\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load the best model\n",
        "import en_core_sci_scibert\n",
        "nlp = en_core_sci_scibert.load()\n"
      ],
      "metadata": {
        "id": "k7QkaKMdlUe9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "# Use your textbook\n",
        "pdf_path = \"/content/Class-XI-Biology.pdf\"\n",
        "raw_text = extract_text_from_pdf(pdf_path)\n"
      ],
      "metadata": {
        "id": "6u2U5HTzlbnT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# List of unwanted terms to remove (case-insensitive)\n",
        "unwanted_terms = [\n",
        "    \"chapter\", \"figure\", \"contain\", \"figures\", \"table\", \"list\", \"example\",\n",
        "    \"figure\", \"image\", \"note\", \"section\", \"text\", \"caption\", \"label\",\n",
        "    \"reference\", \"unit\", \"index\", \"topic\"\n",
        "]\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove Roman numerals (i, ii, iii, iv, etc.) in parentheses\n",
        "    text = re.sub(r'\\([ivxlc]+\\)', '', text)\n",
        "\n",
        "    text = re.sub(r'\\b[ivxlc]+\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove numeric headings like 3.1.3, 3.4, 3.1, etc.\n",
        "    text = re.sub(r'\\d+(\\.\\d+)+', '', text)\n",
        "\n",
        "    # Remove alphabetic labels in parentheses like (a), (b), (c)\n",
        "    text = re.sub(r'\\([a-zA-Z]\\)', '', text)\n",
        "\n",
        "    # Remove unwanted terms (case-insensitive)\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in unwanted_terms])\n",
        "\n",
        "    # Remove any extra spaces left behind\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove duplicate words (preserving order)\n",
        "    seen = set()\n",
        "    cleaned_words = []\n",
        "    for word in text.split():\n",
        "        if word.lower() not in seen:\n",
        "            cleaned_words.append(word)\n",
        "            seen.add(word.lower())\n",
        "\n",
        "    return ' '.join(cleaned_words)\n",
        "raw_text = clean_text(raw_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "W64fL60vt9ep"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Extract biomedical keywords using spaCy + SciSpaCy\n",
        "def extract_keywords_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = set()\n",
        "\n",
        "    # Named biomedical entities (very domain-specific)\n",
        "    for ent in doc.ents:\n",
        "        if len(ent.text.strip()) > 2:\n",
        "            keywords.add(ent.text.strip())\n",
        "\n",
        "    # Add noun chunks (general keyphrases)\n",
        "    for chunk in doc.noun_chunks:\n",
        "        if len(chunk.text.strip()) > 2:\n",
        "            keywords.add(chunk.text.strip())\n",
        "\n",
        "    return sorted(keywords)\n",
        "\n",
        "# Run the keyword extraction\n",
        "keywords = extract_keywords_spacy(raw_text)\n",
        "unique_keywords = sorted(set(keywords))"
      ],
      "metadata": {
        "id": "TEfokGW-liwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2028f367-28d2-4b17-fbb8-f8486f91c30d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Save to file or print sample\n",
        "output_path = \"biology_keywords_using_spacy.txt\"\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(\"Extracted Biology Keywords (SpaCy Only):\\n\")\n",
        "    for word in unique_keywords:\n",
        "        f.write(word + \"\\n\")\n",
        "\n",
        "print(f\"✅ Keywords saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaTGt7Gpllgz",
        "outputId": "02fffdc4-bc53-4af1-ea0d-fbff353fec59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Keywords saved to: biology_keywords_using_spacy.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KeyBERT**"
      ],
      "metadata": {
        "id": "2Czir-jtxijZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install KeyBERT\n",
        "!pip install keybert\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2CCKeqGkloNt",
        "outputId": "e309d897-a2a4-463b-dc22-1b5be2371baa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (4.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.3.8->keybert)\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.31.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.4.26)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.26.1\n",
            "    Uninstalling transformers-4.26.1:\n",
            "      Successfully uninstalled transformers-4.26.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.51.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "0294014c7ed448249d0fa68c40919072"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install transformers==4.35.2\n",
        "!pip install keybert\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTnMx9Mdyewp",
        "outputId": "9fb6370a-f6f6-487e-d305-bfdad72c7361"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers==4.35.2\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2025.4.26)\n",
            "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.35.2 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.35.2\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (4.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.3.8->keybert)\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.31.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.4.26)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy-transformers 1.2.1 requires transformers<4.27.0,>=3.4.0, but you have transformers 4.51.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import necessary libraries\n",
        "from keybert import KeyBERT\n"
      ],
      "metadata": {
        "id": "m8U_2Lv9xw0h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load the model\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')  # Small, fast, and works great\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7woxhEdTx6Xp",
        "outputId": "99083789-e05c-44a3-9a76-0570c039995d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw_text"
      ],
      "metadata": {
        "id": "FxW-nX5AzHEu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Extract keywords/phrases\n",
        "keywords = kw_model.extract_keywords(\n",
        "    text,\n",
        "    keyphrase_ngram_range=(1, 3),     # 1 to 3 words allowed in a phrase\n",
        "    stop_words='english',             # Remove common words\n",
        "    use_maxsum=True,                  # Diverse keywords, avoids redundancy\n",
        "    nr_candidates=20,                 # Number of candidates before filtering\n",
        "    top_n=10                          # Final top 10 key phrases\n",
        ")\n"
      ],
      "metadata": {
        "id": "riUm4KUuyI7S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 6. Print the keywords\n",
        "for keyword, score in keywords:\n",
        "    print(f\"{keyword}: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "_Bfs9Lo3yJ2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb8d002-ef61-4f4f-cf00-5d68b34688ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kingdom plantae popularly: 0.6088\n",
            "biology algae: 0.6121\n",
            "32 biology algae: 0.6121\n",
            "kingdom 35 bryophytes: 0.6138\n",
            "algae bryophytes: 0.6154\n",
            "organisms consider plant: 0.6164\n",
            "understanding plant kingdom: 0.6228\n",
            "placed kingdom cyanobacteria: 0.6435\n",
            "plantae earlier classifications: 0.6492\n",
            "kingdom cyanobacteria referred: 0.6633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"keybert_keywords.txt\", \"w\") as file:\n",
        "    file.write(\"Domain Keywords using KeyBERT:\\n\")\n",
        "    for keyword, score in keywords:\n",
        "        file.write(f\"{keyword}: {score:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "VZo1nALxyOP0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YAKE**"
      ],
      "metadata": {
        "id": "majCH64i4EA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTjJuhIA4Glc",
        "outputId": "210e9eef-4bf2-49e5-840b-43cbebdd37ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yake in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.11/dist-packages (from yake) (8.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from yake) (1.26.4)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.11/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from yake) (3.4.2)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from yake) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from segtok->yake) (2024.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "\n",
        "# Define language and max number of keywords\n",
        "language = \"en\"\n",
        "max_ngram_size = 3\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 50\n",
        "\n",
        "# Initialize the YAKE keyword extractor\n",
        "custom_kw_extractor = yake.KeywordExtractor(\n",
        "    lan=language,\n",
        "    n=max_ngram_size,\n",
        "    dedupLim=deduplication_threshold,\n",
        "    top=numOfKeywords,\n",
        "    features=None\n",
        ")\n"
      ],
      "metadata": {
        "id": "6QyVsJzY4alh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw_text  # from your earlier extraction\n"
      ],
      "metadata": {
        "id": "nr351LV14eFk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = custom_kw_extractor.extract_keywords(text)\n",
        "\n",
        "# Print the keywords\n",
        "for kw, score in keywords:\n",
        "    print(f\"{kw} : {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InXBFtYj4kgo",
        "outputId": "4eaeb1a8-d8bf-4bb7-b5b2-3750b6e5ec3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLANT KINGDOM : 0.0058\n",
            "algae : 0.0066\n",
            "PLANT : 0.0099\n",
            "Red algae : 0.0155\n",
            "Figure : 0.0160\n",
            "called : 0.0164\n",
            "plant body : 0.0171\n",
            "KINGDOM : 0.0194\n",
            "green algae : 0.0200\n",
            "water : 0.0200\n",
            "male : 0.0208\n",
            "plants : 0.0218\n",
            "female : 0.0221\n",
            "form : 0.0232\n",
            "reproduction : 0.0237\n",
            "spores : 0.0260\n",
            "Pteridophytes : 0.0292\n",
            "Brown algae : 0.0316\n",
            "Bryophytes : 0.0333\n",
            "2015-16 : 0.0349\n",
            "Sexual reproduction : 0.0350\n",
            "produce : 0.0355\n",
            "Gymnosperms : 0.0367\n",
            "classification : 0.0371\n",
            "gametes : 0.0387\n",
            "Gametophyte : 0.0393\n",
            "green : 0.0417\n",
            "ovules : 0.0422\n",
            "female gametophytes : 0.0433\n",
            "called green algae : 0.0440\n",
            "sex : 0.0441\n",
            "female sex : 0.0445\n",
            "body : 0.0445\n",
            "Red : 0.0450\n",
            "water algae : 0.0451\n",
            "Sporophyte : 0.0459\n",
            "mosses : 0.0459\n",
            "Angiosperms : 0.0463\n",
            "main plant body : 0.0473\n",
            "male gametes : 0.0474\n",
            "Fresh water algae : 0.0479\n",
            "called red algae : 0.0484\n",
            "forms : 0.0497\n",
            "pollen : 0.0519\n",
            "multicellular : 0.0523\n",
            "leaves : 0.0527\n",
            "cells : 0.0541\n",
            "vegetative : 0.0559\n",
            "sexual : 0.0577\n",
            "develops : 0.0578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake-nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ivXCag54mR2",
        "outputId": "79f20e0c-108c-49ab-e3ee-13850ddbc389"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.11/dist-packages (1.0.6)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from rake-nltk) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3xX1lGM6CZ4",
        "outputId": "6d433d59-88e3-437d-d5b9-259558b0b145"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake\n",
        "\n",
        "\n",
        "# Initialize Rake with English stopwords\n",
        "rake_extractor = Rake()\n"
      ],
      "metadata": {
        "id": "AsiSvcMf5RPH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = raw_text  # Already extracted using PyMuPDF\n"
      ],
      "metadata": {
        "id": "Kk_ShvRH5Thv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed the text to RAKE\n",
        "rake_extractor.extract_keywords_from_text(text)\n",
        "\n",
        "# Get ranked keywords (highest ranked first)\n",
        "rake_keywords = rake_extractor.get_ranked_phrases()\n",
        "\n",
        "# Optional: remove duplicates and filter out generic terms\n",
        "unwanted_terms = ['chapter', 'figure', 'table', 'contain', 'includes', 'example']\n",
        "\n",
        "# Clean keywords\n",
        "cleaned_rake_keywords = list({kw for kw in rake_keywords if not any(term in kw.lower() for term in unwanted_terms)})\n",
        "\n",
        "# Display the top 50 cleaned keywords\n",
        "for kw in cleaned_rake_keywords[:50]:\n",
        "    print(kw)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_5-D3ad5hWu",
        "outputId": "cc87b4d1-be3d-4411-a868-c02059696c9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meiosis\n",
            "characterised\n",
            "largely aquatic\n",
            "gametophytic plant body\n",
            "celled egg apparatus – one egg cell\n",
            "sloth bear ).\n",
            "form new individuals\n",
            "gametophyte\n",
            "mosses\n",
            "taxa\n",
            "considered\n",
            "evolutionarily\n",
            "monera\n",
            "proteins\n",
            "fragmentation\n",
            "6 life cycle\n",
            "two cotyledons\n",
            "useful\n",
            "reproduce asexually\n",
            "two fusions\n",
            "female gamete\n",
            "fusion\n",
            "secondary protonema\n",
            "chemical constituents\n",
            "pteridophytes\n",
            "cytotaxonomy\n",
            "development\n",
            "brown algae\n",
            "mouth\n",
            "highly variable\n",
            "5 ).\n",
            "sources\n",
            "formation\n",
            "stage bears\n",
            "fungi\n",
            "developed\n",
            "possess chlorophyll\n",
            "produced either\n",
            "produce\n",
            "amount\n",
            "frond\n",
            "upright\n",
            "shady places\n",
            "unicellular\n",
            "fragment develops\n",
            "5 angiosperms\n",
            "cycas male cones\n",
            "thalli\n",
            "colour\n",
            "germination gives rise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"rake_keywords.txt\", \"w\") as f:\n",
        "    for kw in cleaned_rake_keywords:\n",
        "        f.write(kw + \"\\n\")\n"
      ],
      "metadata": {
        "id": "Gm95KG_R5krl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install and Import\n",
        "!pip install rake-nltk\n",
        "\n",
        "from rake_nltk import Rake\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 2. Load Text (your PDF already loaded as raw_text)\n",
        "text = raw_text\n",
        "\n",
        "# 3. Initialize RAKE\n",
        "rake = Rake()\n",
        "\n",
        "# 4. Extract Keywords\n",
        "rake.extract_keywords_from_text(text)\n",
        "keywords = rake.get_ranked_phrases()\n",
        "\n",
        "# 5. Custom Filtering\n",
        "unwanted_terms = ['chapter', 'figure', 'table', 'contain', 'includes', 'example', 'iii', 'ii', 'i', 'a-i', 'a-ii', 'a-iii']\n",
        "filtered_keywords = []\n",
        "\n",
        "for kw in keywords:\n",
        "    kw_lower = kw.lower()\n",
        "    if not any(term in kw_lower for term in unwanted_terms) and len(kw.strip()) > 2:\n",
        "        filtered_keywords.append(kw.strip())\n",
        "\n",
        "# 6. Remove Near Duplicates\n",
        "unique_keywords = list(set(filtered_keywords))\n",
        "unique_keywords.sort()\n",
        "\n",
        "# 7. Display Top Keywords\n",
        "for keyword in unique_keywords[:50]:\n",
        "    print(keyword)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r13k6mEF6Xap",
        "outputId": "1f664155-2c1b-4a62-95c1-b3b4c13db4ea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.11/dist-packages (1.0.6)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from rake-nltk) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n",
            "1 ).\n",
            "1 algae\n",
            "1 algae 3\n",
            "1 algae algae\n",
            "1 chlorophyceae\n",
            "100 metres\n",
            "100 metres ).\n",
            "1969\n",
            "1a ).\n",
            "1b ).\n",
            "2 ).\n",
            "2 bryophytes\n",
            "2 bryophytes 3\n",
            "2 mosses\n",
            "2 phaeophyceae\n",
            "2015\n",
            "3 ).\n",
            "3 rhodophyceae\n",
            "4 ).\n",
            "4 gymnosperms\n",
            "4 gymnosperms 3\n",
            "5 ).\n",
            "absent fresh water algae\n",
            "adapted\n",
            "agar\n",
            "algae\n",
            "algae may store food\n",
            "algae reproduce\n",
            "also\n",
            "also frequently grown\n",
            "also occur\n",
            "also referred\n",
            "also used\n",
            "among\n",
            "amount\n",
            "anatomy\n",
            "anther\n",
            "anthers\n",
            "apex\n",
            "asexual\n",
            "asexual buds\n",
            "assumes\n",
            "attached\n",
            "banks\n",
            "bare rocks\n",
            "bark\n",
            "based\n",
            "bear two laterally attached flagella\n",
            "bears two\n",
            "become\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhtMzfL16YSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}